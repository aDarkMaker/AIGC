import sys
import os
from pathlib import Path
import argparse
import json
from typing import Dict, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹è·¯å¾„åˆ°ç³»ç»Ÿè·¯å¾„
project_root = Path(__file__).parent.absolute()
sys.path.insert(0, str(project_root))

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from vivo_rag_system.src import RAGEngine  # é€šè¿‡åŒ…çš„__init__.pyå¯¼å…¥
from analysis_part.analysis import EnhancedAnalyzer

# æ·»åŠ è®­ç»ƒç›¸å…³å¯¼å…¥
def load_training_modules():
    """å®‰å…¨åŠ è½½è®­ç»ƒæ¨¡å—"""
    try:
        # æ£€æŸ¥å¿…è¦çš„ä¾èµ–
        import torch
        import transformers
        
        # å°† Law-Train ç›®å½•æ·»åŠ åˆ°è·¯å¾„
        law_train_path = project_root / "Law-Train"
        if str(law_train_path) not in sys.path:
            sys.path.insert(0, str(law_train_path))
        
        # å¯¼å…¥è®­ç»ƒæ¨¡å—
        from src.train import LegalBertTrainer, LegalCorpusDataset
        print("âœ“ è®­ç»ƒæ¨¡å—å·²æˆåŠŸåŠ è½½")
        return True, LegalBertTrainer, LegalCorpusDataset
        
    except ImportError as e:
        print(f"âš ï¸ è­¦å‘Š: è®­ç»ƒæ¨¡å—ä¸å¯ç”¨ - {str(e)}")
        print("å¯èƒ½çš„åŸå› :")
        print("  1. ç¼ºå°‘å¿…è¦çš„ä¾èµ–åŒ… (torch, transformers ç­‰)")
        print("  2. Law-Train/src/train.py æ–‡ä»¶ä¸å­˜åœ¨æˆ–æœ‰é”™è¯¯") 
        print("  3. è¯·å…ˆè¿è¡Œ: pip install -r Law-Train/requirements.txt")
        return False, None, None
    except Exception as e:
        print(f"âš ï¸ åŠ è½½è®­ç»ƒæ¨¡å—æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}")
        return False, None, None

# å°è¯•åŠ è½½è®­ç»ƒæ¨¡å—
TRAINING_AVAILABLE, LegalBertTrainer, LegalCorpusDataset = load_training_modules()

class IntegratedAnalysisSystem:
    """é›†æˆåˆ†æç³»ç»Ÿ"""
    def __init__(self):
        self.rag_engine = RAGEngine()
        self.legal_analyzer = EnhancedAnalyzer()
        self.trainer = None
        
        # åˆå§‹åŒ–è®­ç»ƒå™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if TRAINING_AVAILABLE and LegalBertTrainer:
            try:
                self.trainer = LegalBertTrainer()
                print("âœ“ è®­ç»ƒå™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ è®­ç»ƒå™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
                # ä¸åœ¨è¿™é‡Œä¿®æ”¹å…¨å±€å˜é‡ï¼Œè€Œæ˜¯åœ¨ç±»çº§åˆ«æ ‡è®°
    
    def analyze_document(self, text: str, domain: str = 'privacy', use_professional_kb: bool = False) -> Dict[str, Any]:
        """å®Œæ•´æ–‡æ¡£åˆ†æ"""
        # RAG ç³»ç»Ÿåˆ†æ
        rag_results = self.rag_engine.process_query(text, use_professional_kb=use_professional_kb) # ä¼ é€’å‚æ•°
        
        # æ³•å¾‹åˆ†æ
        legal_results = self.legal_analyzer.analyze_document(text, domain)
        
        # åˆå¹¶ç»“æœ
        return {
            "rag_analysis": {
                "keywords": rag_results["keywords"],
                "summary": rag_results["summary"],
                "related_context": rag_results["context"]
            },
            "legal_analysis": legal_results
        }
    
    def save_results(self, results: Dict[str, Any], output_dir: str):
        """ä¿å­˜åˆ†æç»“æœ"""
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜JSONæ ¼å¼ç»“æœ
        json_path = os.path.join(output_dir, "analysis_results.json")
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
            
        print(f"åˆ†æç»“æœå·²ä¿å­˜è‡³: {json_path}")
    
    def train_model(self, data_path: str, output_dir: str, **kwargs):
        """è®­ç»ƒæ³•å¾‹æ–‡æœ¬æ¨¡å‹"""
        if not TRAINING_AVAILABLE or not LegalCorpusDataset:
            print("âŒ é”™è¯¯: è®­ç»ƒæ¨¡å—ä¸å¯ç”¨")
            print("è¯·å…ˆå®‰è£…è®­ç»ƒä¾èµ–: pip install -r Law-Train/requirements.txt")
            return False
            
        try:
            print("ğŸš€ å¼€å§‹è®­ç»ƒæ³•å¾‹æ–‡æœ¬æ¨¡å‹...")
            
            # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(data_path):
                print(f"âŒ é”™è¯¯: è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
                return False
            
            # å‡†å¤‡æ•°æ®é›†
            dataset = LegalCorpusDataset(data_path, self.trainer.tokenizer, augment=True)
            
            # è®­ç»ƒæ¨¡å‹
            self.trainer.train(
                dataset, 
                output_dir,
                batch_size=kwargs.get('batch_size', 8),
                num_epochs=kwargs.get('num_epochs', 3),
                learning_rate=kwargs.get('learning_rate', 2e-5)
            )
            
            # ä¿å­˜æ¨¡å‹
            self.trainer.save_model(output_dir)
            
            print(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œå·²ä¿å­˜è‡³: {output_dir}")
            return True
            
        except Exception as e:
            print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            import traceback
            traceback.print_exc()
            return False
    
    def evaluate_model(self, test_data_path: str, model_path: str):
        """è¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡å‹"""
        if not TRAINING_AVAILABLE:
            print("é”™è¯¯: è®­ç»ƒæ¨¡å—ä¸å¯ç”¨")
            return None
            
        try:
            print("å¼€å§‹è¯„ä¼°æ¨¡å‹...")
            # TODO: å®ç°æ¨¡å‹è¯„ä¼°é€»è¾‘
            print("æ¨¡å‹è¯„ä¼°å®Œæˆ")
            return {}
        except Exception as e:
            print(f"è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            return None
    
    def interactive_mode(self):
        """äº¤äº’å¼åˆ†ææ¨¡å¼"""
        print("=== æ¬¢è¿ä½¿ç”¨é›†æˆåˆ†æç³»ç»Ÿ ===")
        print("æ”¯æŒçš„åˆ†æé¢†åŸŸ: privacy(éšç§), intellectual_property(çŸ¥è¯†äº§æƒ), contract(åˆåŒ)")
        
        while True:
            print("\nè¯·é€‰æ‹©æ“ä½œ:")
            print("1. åˆ†ææ–‡æœ¬æ–‡ä»¶")
            print("2. è¾“å…¥æ–‡æœ¬è¿›è¡Œåˆ†æ")
            if TRAINING_AVAILABLE:
                print("3. è®­ç»ƒæ¨¡å‹ âœ“")
                print("4. è¯„ä¼°æ¨¡å‹ âœ“") 
                print("5. è¯Šæ–­è®­ç»ƒç¯å¢ƒ")
            else:
                print("3. è®­ç»ƒæ¨¡å‹ âŒ (ä¸å¯ç”¨)")
                print("4. è¯„ä¼°æ¨¡å‹ âŒ (ä¸å¯ç”¨)")
                print("5. è¯Šæ–­è®­ç»ƒç¯å¢ƒ ğŸ”§")
            print("6. é€€å‡º")
            
            choice = input("è¯·è¾“å…¥é€‰é¡¹ç¼–å·: ").strip()
            
            if choice == "1":
                file_path = input("è¯·è¾“å…¥æ–‡ä»¶è·¯å¾„: ").strip()
                domain = input("è¯·è¾“å…¥åˆ†æé¢†åŸŸ (é»˜è®¤ä¸ºprivacy): ").strip() or "privacy"
                
                try:
                    with open(file_path, "r", encoding="utf-8") as f:
                        text = f.read()
                    results = self.analyze_document(text, domain)
                    self.save_results(results, "analysis_output")
                except Exception as e:
                    print(f"âŒ é”™è¯¯: {str(e)}")
                    
            elif choice == "2":
                print("è¯·è¾“å…¥è¦åˆ†æçš„æ–‡æœ¬ (è¾“å…¥ç©ºè¡Œç»“æŸ):")
                lines = []
                while True:
                    line = input()
                    if not line:
                        break
                    lines.append(line)
                
                if lines:
                    domain = input("è¯·è¾“å…¥åˆ†æé¢†åŸŸ (é»˜è®¤ä¸ºprivacy): ").strip() or "privacy"
                    text = "\n".join(lines)
                    results = self.analyze_document(text, domain)
                    self.save_results(results, "analysis_output")
                    
            elif choice == "3":
                if not TRAINING_AVAILABLE:
                    print("âŒ è®­ç»ƒåŠŸèƒ½ä¸å¯ç”¨ï¼Œè¯·é€‰æ‹©é€‰é¡¹5è¿›è¡Œè¯Šæ–­")
                    continue
                    
                data_path = input("è¯·è¾“å…¥è®­ç»ƒæ•°æ®è·¯å¾„: ").strip()
                output_dir = input("è¯·è¾“å…¥æ¨¡å‹ä¿å­˜è·¯å¾„ (é»˜è®¤: ./Law-Train/model/trained_model): ").strip()
                if not output_dir:
                    output_dir = "./Law-Train/model/trained_model"
                
                # è®­ç»ƒå‚æ•°
                batch_size = input("æ‰¹å¤„ç†å¤§å° (é»˜è®¤8): ").strip()
                batch_size = int(batch_size) if batch_size.isdigit() else 8
                
                epochs = input("è®­ç»ƒè½®æ•° (é»˜è®¤3): ").strip()
                epochs = int(epochs) if epochs.isdigit() else 3
                
                self.train_model(
                    data_path, 
                    output_dir,
                    batch_size=batch_size,
                    num_epochs=epochs
                )
                
            elif choice == "4":
                if not TRAINING_AVAILABLE:
                    print("âŒ è¯„ä¼°åŠŸèƒ½ä¸å¯ç”¨ï¼Œè¯·é€‰æ‹©é€‰é¡¹5è¿›è¡Œè¯Šæ–­")
                    continue
                    
                test_data_path = input("è¯·è¾“å…¥æµ‹è¯•æ•°æ®è·¯å¾„: ").strip()
                model_path = input("è¯·è¾“å…¥æ¨¡å‹è·¯å¾„: ").strip()
                self.evaluate_model(test_data_path, model_path)
                
            elif choice == "5":
                self.diagnose_training_setup()
                
            elif choice == "6":
                print("æ„Ÿè°¢ä½¿ç”¨!")
                break
            else:
                print("âŒ æ— æ•ˆçš„é€‰é¡¹ï¼Œè¯·é‡æ–°é€‰æ‹©")
    
    def diagnose_training_setup(self):
        """è¯Šæ–­è®­ç»ƒç¯å¢ƒè®¾ç½®"""
        print("ğŸ” æ­£åœ¨è¯Šæ–­è®­ç»ƒç¯å¢ƒ...")
        print("-" * 50)
        
        # æ£€æŸ¥Pythonç‰ˆæœ¬
        print(f"Pythonç‰ˆæœ¬: {sys.version}")
        
        # æ£€æŸ¥å…³é”®ä¾èµ–
        dependencies = [
            'torch', 'transformers', 'numpy', 'pandas', 
            'scikit-learn', 'tqdm', 'jieba'
        ]
        
        missing_deps = []
        for dep in dependencies:
            try:
                __import__(dep)
                print(f"âœ“ {dep}: å·²å®‰è£…")
            except ImportError:
                print(f"âŒ {dep}: æœªå®‰è£…")
                missing_deps.append(dep)
        
        # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
        required_files = [
            'Law-Train/src/train.py',
            'Law-Train/src/__init__.py', 
            'Law-Train/requirements.txt'
        ]
        
        for file_path in required_files:
            full_path = project_root / file_path
            if full_path.exists():
                print(f"âœ“ {file_path}: å­˜åœ¨")
            else:
                print(f"âŒ {file_path}: ä¸å­˜åœ¨")
        
        # æä¾›è§£å†³å»ºè®®
        if missing_deps:
            print("\nğŸ’¡ å»ºè®®æ“ä½œ:")
            print("1. å®‰è£…ç¼ºå¤±çš„ä¾èµ–åŒ…:")
            print(f"   pip install {' '.join(missing_deps)}")
            print("2. æˆ–è€…å®‰è£…å®Œæ•´è®­ç»ƒç¯å¢ƒ:")
            print("   pip install -r Law-Train/requirements.txt")
        
        print("-" * 50)

def main():
    parser = argparse.ArgumentParser(description="é›†æˆæ–‡æœ¬åˆ†æç³»ç»Ÿ")
    parser.add_argument("--file", "-f", help="è¦åˆ†æçš„æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„Target.txtï¼‰")
    parser.add_argument("--domain", "-d", default="privacy",
                      choices=["privacy", "intellectual_property", "contract"],
                      help="åˆ†æé¢†åŸŸ (é»˜è®¤: privacy)")
    parser.add_argument("--interactive", "-i", action="store_true",
                      help="å¯åŠ¨äº¤äº’å¼æ¨¡å¼")
    parser.add_argument("--diagnose", action="store_true",
                      help="è¯Šæ–­è®­ç»ƒç¯å¢ƒè®¾ç½®")
    # æ–°å¢è®­ç»ƒç›¸å…³å‚æ•°
    parser.add_argument("--train", action="store_true",
                      help="è®­ç»ƒæ¨¡å¼")
    parser.add_argument("--train-data", 
                      help="è®­ç»ƒæ•°æ®è·¯å¾„")
    parser.add_argument("--model-output", default="./Law-Train/model/trained_model",
                      help="æ¨¡å‹è¾“å‡ºç›®å½•")
    parser.add_argument("--batch-size", type=int, default=8,
                      help="æ‰¹å¤„ç†å¤§å°")
    parser.add_argument("--epochs", type=int, default=3,
                      help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--learning-rate", type=float, default=2e-5,
                      help="å­¦ä¹ ç‡")
    
    args = parser.parse_args()
    system = IntegratedAnalysisSystem()
    
    if args.diagnose:
        system.diagnose_training_setup()
        return
    
    if args.train:
        if not TRAINING_AVAILABLE:
            print("é”™è¯¯: è®­ç»ƒæ¨¡å—ä¸å¯ç”¨")
            sys.exit(1)
        
        if not args.train_data:
            print("é”™è¯¯: è®­ç»ƒæ¨¡å¼éœ€è¦æŒ‡å®šè®­ç»ƒæ•°æ®è·¯å¾„ (--train-data)")
            sys.exit(1)
            
        success = system.train_model(
            args.train_data,
            args.model_output,
            batch_size=args.batch_size,
            num_epochs=args.epochs,
            learning_rate=args.learning_rate
        )
        
        if success:
            print("è®­ç»ƒå®Œæˆï¼")
        else:
            print("è®­ç»ƒå¤±è´¥ï¼")
            sys.exit(1)
            
    elif args.interactive:
        system.interactive_mode()
    else:
        try:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šæ–‡ä»¶ï¼Œåˆ™ä½¿ç”¨ Target.txt
            input_file = args.file if args.file else os.path.join(project_root, "Target.txt")
            
            if not os.path.exists(input_file):
                print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {input_file}")
                sys.exit(1)
                
            with open(input_file, "r", encoding="utf-8") as f:
                text = f.read()
                
            print(f"æ­£åœ¨åˆ†ææ–‡ä»¶: {input_file}")
            results = system.analyze_document(text, args.domain)
            system.save_results(results, "analysis_output")
            print("åˆ†æå®Œæˆï¼")
            
        except Exception as e:
            print(f"é”™è¯¯: {str(e)}")
            sys.exit(1)

if __name__ == "__main__":
    main()